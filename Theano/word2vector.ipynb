{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "utilities.py:30: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  train_input = reshape(data.trainData[0:D, 0:N * M], (D, N, M));\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.py:224: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return reshape(newshape, order=order)\n",
      "utilities.py:33: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  train_target = reshape(data.trainData[D, 0:N * M], (1, N, M));\n"
     ]
    }
   ],
   "source": [
    "from utilities import load_data\n",
    "batchsize = 100\n",
    "[train_input, train_target, valid_input, valid_target, test_input, test_target, vocab] = load_data(batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 100, 3725)\n",
      "(3, 100)\n",
      "(1, 100)\n",
      "[192 151  22]\n",
      "54\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'numpy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-61ae7be14d64>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_input\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_target\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'numpy' is not defined"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_input[:,:,0].shape)\n",
    "print(train_target[:,:,0].shape)\n",
    "print(valid_input[:,10])\n",
    "print(valid_target[10])\n",
    "numpy.max(valid_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 15)\n",
      "[  40001.   40002.   40003.   40004.   40005.  140001.  140002.  140003.\n",
      "  140004.  140005.  170001.  170002.  170003.  170004.  170005.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "\n",
    "class NextWord(object):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        numpy_rng,\n",
    "        theano_rng = None,\n",
    "        input = None,\n",
    "        output = None,\n",
    "        n_embed = 200,\n",
    "        n_hidden = 200,\n",
    "        wW2E = None,\n",
    "        wE2H = None,\n",
    "        wH2O = None,\n",
    "        bH = None,\n",
    "        bO = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the Object by specifying the number of visible units (the\n",
    "        dimension d of the input), the number of hidden units (the dimension\n",
    "        d' of the latent or hidden space).\n",
    "        The constructor also receives symbolic variables for the input, weights and\n",
    "        bias.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: number random generator used to generate weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                     generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type input: theano.tensor.TensorType\n",
    "        :param input: a symbolic description of the input or None for standalone model\n",
    "\n",
    "        :type n_embed: int\n",
    "        :param n_embed: number of word-embedding units\n",
    "\n",
    "        :type n_hidden: int\n",
    "        :param n_hidden: number of hidden units\n",
    "\n",
    "        :type wW2E: theano.tensor.TensorType\n",
    "        :param wW2E: Theano variable pointing to a set of weights that should be\n",
    "                      shared across model architecture;\n",
    "                      if model is standalone set this to None\n",
    "        \n",
    "        :type wE2H: theano.tensor.TensorType\n",
    "        :param wE2H: Theano variable pointing to a set of weights that should be\n",
    "                      shared across model architecture;\n",
    "                      if model is standalone set this to None\n",
    "\n",
    "        :type bH: theano.tensor.TensorType\n",
    "        :param bH: Theano variable pointing to a set of biases values (for \n",
    "                    visible units) that should be shared across model \n",
    "                    architecture; if model is standalone set this to None\n",
    "\n",
    "        :type bO: theano.tensor.TensorType\n",
    "        :param bO: Theano variable pointing to a set of biases values (for \n",
    "                    visible units) that should be shared across model \n",
    "                    architecture; if model is standalone set this to None\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        self.numwords = 3 # i.e. we use 3-grams to predict the 4th word\n",
    "        #batchsize\n",
    "        self.vocab_size = 250\n",
    "        #numhid1\n",
    "        #numhid2 # numhid2 is the number of hidden units.\n",
    "        self.n_embed = n_embed\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = self.vocab_size #self.n_embed\n",
    "        #numhid1 = n_embed * numwords\n",
    "\n",
    "        # create a Theano random generator that gives symbolic random values\n",
    "        if not theano_rng:\n",
    "            theano_rng = RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "            \n",
    "        #word_embedding_weights: Word embedding as a matrix of size\n",
    "        #vocab_size X numhid1, where vocab_size is the size of the vocabulary\n",
    "        #numhid1 is the dimensionality of the embedding space.\n",
    "        \n",
    "        if not wW2E: # word_embedding_weights as matrix of size vocab_size X numhid1\n",
    "            # Initialized with uniform sample\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_wW2E = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (self.vocab_size + n_embed)),\n",
    "                    high=4 * numpy.sqrt(6. / (self.vocab_size + n_embed)),\n",
    "                    size=(self.vocab_size, n_embed)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            wW2E = theano.shared(value=initial_wW2E, name='wW2E', borrow=True)\n",
    "\n",
    "        if not wE2H: # embed_to_hid_weights as a matrix of size numhid1*numwords X numhid2\n",
    "            # Initialized with uniform sample\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_wE2H = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_embed*self.numwords + n_hidden)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_embed*self.numwords + n_hidden)),\n",
    "                    size=(n_embed*self.numwords, n_hidden)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            wE2H = theano.shared(value=initial_wE2H, name='wE2H', borrow=True)\n",
    "            \n",
    "        if not bH: # hid_bias: Bias of the hidden layer as a matrix of size numhid2 X 1.\n",
    "            bH = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    n_hidden,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='bH',\n",
    "                borrow=True\n",
    "            )\n",
    "            \n",
    "        if not wH2O: # hid_to_output_weights\n",
    "            # Initialized with uniform sample\n",
    "            # converted using asarray to dtype\n",
    "            # theano.config.floatX so that the code is runable on GPU\n",
    "            initial_wH2O = numpy.asarray(\n",
    "                numpy_rng.uniform(\n",
    "                    low=-4 * numpy.sqrt(6. / (n_hidden + self.n_output)),\n",
    "                    high=4 * numpy.sqrt(6. / (n_hidden + self.n_output)),\n",
    "                    size=(n_hidden, self.n_output)\n",
    "                ),\n",
    "                dtype=theano.config.floatX\n",
    "            )\n",
    "            wH2O = theano.shared(value=initial_wH2O, name='wH2O', borrow=True)\n",
    "        \n",
    "        if not bO: # output_bias: Bias of the output layer as a matrix of size vocab_size X 1.\n",
    "            bO = theano.shared(\n",
    "                value=numpy.zeros(\n",
    "                    self.n_output,\n",
    "                    dtype=theano.config.floatX\n",
    "                ),\n",
    "                name='bO',\n",
    "                borrow=True\n",
    "            )\n",
    "\n",
    "\n",
    "        self.wW2E = wW2E\n",
    "        self.wE2H = wE2H\n",
    "        self.wH2O = wH2O\n",
    "        # b corresponds to the bias of the hidden activations\n",
    "        self.bH = bH\n",
    "        self.bO = bO\n",
    "        self.theano_rng = theano_rng\n",
    "        # if no input is given, generate a variable representing the input\n",
    "        if input is None:\n",
    "            # we use a matrix because we expect a minibatch of several examples\n",
    "            self.x = T.dmatrix(name='input')\n",
    "        else:\n",
    "            self.x = input\n",
    "            \n",
    "        if output is None:\n",
    "            # we use a vector because we expect a minibatch of several examples\n",
    "            self.y = T.dvector(name='output')\n",
    "        else:\n",
    "            self.y = output\n",
    "\n",
    "        self.params = [self.wW2E, self.wE2H, self.wH2O, self.bH, self.bO]\n",
    "\n",
    "    def get_embed_values(self, words):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        #embedding_layer_state = reshape(...\n",
    "        #word_embedding_weights(reshape(input_batch, 1, []),:)',...\n",
    "        #n_embed * numwords, []);\n",
    "        # see : http://stackoverflow.com/questions/33947726/indexing-tensor-with-index-matrix-in-theano\n",
    "        \n",
    "        A = self.wW2E # dim = (vocab, embed)\n",
    "        #W = words # dim = (batchsize, nbwords)\n",
    "        \n",
    "        return self.wW2E[words,:].reshape((-1,self.numwords*self.n_embed))\n",
    "\n",
    "        AA = T.matrix()\n",
    "        WW = T.imatrix()\n",
    "        #CC = AA[WW,:].reshape((batchsize,nbwords*self.n_embed))\n",
    "        CC = AA[WW,:].reshape((-1,self.numwords*self.n_embed))\n",
    "        print('a')\n",
    "        print(type(AA))\n",
    "        print(type(self.wW2E))\n",
    "        print(type(WW))\n",
    "        print(type(words))\n",
    "        f = theano.function([AA, WW], CC, allow_input_downcast=True)\n",
    "        print('b')\n",
    "        return f(A.astype(theano.config.floatX), words)\n",
    "    \n",
    "    def get_hidden_values(self, embed):\n",
    "        \"\"\" Computes the values of the hidden layer \"\"\"\n",
    "        print('H')\n",
    "        return T.nnet.sigmoid(T.dot(embed, self.wE2H) + self.bH)\n",
    "\n",
    "    def get_output(self, hidden):\n",
    "        \"\"\" Computes the reconstructed input given the values of the hidden layer \"\"\"\n",
    "        print('O')\n",
    "        return T.nnet.softmax(T.dot(hidden, self.wH2O) + self.bO)\n",
    "\n",
    "    def get_cost_updates(self, learning_rate):\n",
    "        \"\"\" This function computes the cost and the updates for one trainng step of the model \"\"\"\n",
    "        embed = self.get_embed_values(self.x)\n",
    "        hidden = self.get_hidden_values(embed)\n",
    "        output = self.get_output(hidden)\n",
    "        # note : we sum over the size of a datapoint; if we are using\n",
    "        #        minibatches, L will be a vector, with one entry per\n",
    "        #        example in minibatch\n",
    "        #L = - T.sum(self.y * T.log(output) + (1 - self.y) * T.log(1 - output), axis=1)\n",
    "        L = - T.sum(output) - T.sum(self.y)\n",
    "        # note : L is now a vector, where each element is the\n",
    "        #        cross-entropy cost of the reconstruction of the\n",
    "        #        corresponding example of the minibatch. We need to\n",
    "        #        compute the average of all these to get the cost of\n",
    "        #        the minibatch\n",
    "        #cost = T.mean(L)\n",
    "        cost = L\n",
    "        print(cost)\n",
    "\n",
    "        # compute the gradients of the cost with respect to its parameters\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        # generate the list of updates\n",
    "        updates = [\n",
    "            (param, param - learning_rate * gparam)\n",
    "            for param, gparam in zip(self.params, gparams)\n",
    "        ] # WARNING : no momentum at present\n",
    "\n",
    "        return (cost, updates)\n",
    "\n",
    "\n",
    "# Temporary snippet to check dimension consistency\n",
    "# Reshape demo based on :\n",
    "# http://stackoverflow.com/questions/33947726/indexing-tensor-with-index-matrix-in-theano\n",
    "\n",
    "from random import randint\n",
    "\n",
    "nbwords = 3\n",
    "vocab = 30\n",
    "embed = 5\n",
    "batchsize = 10\n",
    "\n",
    "A = numpy.array([[int(str(i+1)+'000'+str(j+1)) for j in range(embed)] for i in range(vocab)]).reshape(vocab, embed)\n",
    "B = numpy.array([randint(0,vocab-1) for i in range(nbwords*batchsize)]).reshape(batchsize, nbwords)\n",
    "\n",
    "AA = T.matrix()\n",
    "BB = T.imatrix()\n",
    "\n",
    "#CC = AA[T.arange(AA.shape[0]).reshape((-1, 1)), T.arange(AA.shape[1]), BB]\n",
    "#CC = AA[BB,:]\n",
    "#CC = AA[BB,:].reshape((batchsize,nbwords*embed))\n",
    "CC = AA[BB,:].reshape((-1,nbwords*embed))\n",
    "\n",
    "f = theano.function([AA, BB], CC, allow_input_downcast=True)\n",
    "\n",
    "D = f(A.astype(theano.config.floatX), B)\n",
    "\n",
    "print(D.shape)\n",
    "print(D[0])\n",
    "\n",
    "\n",
    "# Temporary snippet to check the class is correctly defined\n",
    "x = T.imatrix('x')\n",
    "y = T.imatrix('y')\n",
    "\n",
    "rng = numpy.random.RandomState(123)\n",
    "theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "nw = NextWord(\n",
    "        numpy_rng = rng,\n",
    "        theano_rng = theano_rng,\n",
    "        input = x,\n",
    "        output = y,\n",
    "        n_embed = 50,\n",
    "        n_hidden = 100,\n",
    "        wW2E = None,\n",
    "        wE2H = None,\n",
    "        wH2O = None,\n",
    "        bH = None,\n",
    "        bO = None\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def test_NextWord(learning_rate=0.1, training_epochs=5, batch_size=100):\n",
    "\n",
    "    \"\"\"\n",
    "    :type learning_rate: float\n",
    "    :param learning_rate: learning rate used for training the DeNosing\n",
    "                          AutoEncoder\n",
    "\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: number of epochs used for training\n",
    "\n",
    "    :type dataset: string\n",
    "    :param dataset: path to the picked dataset\n",
    "\n",
    "    \"\"\"\n",
    "    #datasets = load_data(dataset)\n",
    "    #train_set_x, train_set_y = datasets[0]\n",
    "    [train_input, train_target, valid_input, valid_target, test_input, test_target, vocab] = load_data(batch_size)\n",
    "\n",
    "    \"\"\" Function that loads the dataset into shared variables\n",
    "\n",
    "        The reason we store our dataset in shared variables is to allow\n",
    "        Theano to copy it into the GPU memory (when code is run on GPU).\n",
    "        Since copying data into the GPU is slow, copying a minibatch everytime\n",
    "        is needed (the default behaviour if the data is not in a shared\n",
    "        variable) would lead to a large decrease in performance.\n",
    "    \"\"\"\n",
    "    borrow = True\n",
    "    shared_x = theano.shared(numpy.asarray(train_input, dtype=theano.config.floatX),borrow=borrow)\n",
    "    shared_y = theano.shared(numpy.asarray(train_target, dtype=theano.config.floatX), borrow=borrow)\n",
    "    # When storing data on the GPU it has to be stored as floats\n",
    "    # therefore we will store the labels as ``floatX`` as well\n",
    "    # (``shared_y`` does exactly that). But during our computations\n",
    "    # we need them as ints (we use labels as index, and if they are\n",
    "    # floats it doesn't make sense) therefore instead of returning\n",
    "    # ``shared_y`` we will have to cast it to int. This little hack\n",
    "    # lets ous get around this issue\n",
    "    train_set_x = T.cast(shared_x, 'int32')\n",
    "    train_set_y = T.cast(shared_y, 'int32')\n",
    "    \n",
    "    \n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    #n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "    n_train_batches = min(500, train_input.shape[-1]) # 5 to test\n",
    "\n",
    "    # start-snippet-2\n",
    "    # allocate symbolic variables for the data\n",
    "    index = T.lscalar()    # index to a [mini]batch\n",
    "    x = T.imatrix('x')\n",
    "    y = T.imatrix('y')\n",
    "\n",
    "    rng = numpy.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    nw = NextWord(\n",
    "        numpy_rng = rng,\n",
    "        theano_rng = theano_rng,\n",
    "        input = x,\n",
    "        output = y,\n",
    "        n_embed = 50,\n",
    "        n_hidden = 200\n",
    "    )\n",
    "    print(nw)\n",
    "    cost, updates = nw.get_cost_updates(learning_rate=learning_rate)\n",
    "    print(cost, updates)\n",
    "    train_nw = theano.function(\n",
    "        [index],\n",
    "        cost,\n",
    "        updates=updates,\n",
    "        givens={\n",
    "            #x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "            x: train_set_x[:,:,index],\n",
    "            #y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "            y: train_set_y[:,:,index]\n",
    "        }\n",
    "    )\n",
    "    print(train_nw)\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    ############\n",
    "    # TRAINING #\n",
    "    ############\n",
    "\n",
    "    # go through training epochs\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for batch_index in range(n_train_batches):\n",
    "            #print(batch_index)\n",
    "            c.append(train_nw(batch_index))\n",
    "\n",
    "        print('Training epoch %d, cost ' % epoch, numpy.mean(c, dtype='float64'))\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    training_time = (end_time - start_time)\n",
    "\n",
    "    print('The code ran for %.2fm' % ((training_time) / 60.))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.NextWord object at 0x7ff49f7be510>\n",
      "H\n",
      "O\n",
      "Elemwise{sub,no_inplace}.0\n",
      "Elemwise{sub,no_inplace}.0 [(wW2E, Elemwise{sub,no_inplace}.0), (wE2H, Elemwise{sub,no_inplace}.0), (wH2O, Elemwise{sub,no_inplace}.0), (bH, Elemwise{sub,no_inplace}.0), (bO, Elemwise{sub,no_inplace}.0)]\n",
      "<theano.compile.function_module.Function object at 0x7ff49f9ac810>\n",
      "Training epoch 0, cost  -12425.218\n",
      "Training epoch 1, cost  -12425.218\n",
      "Training epoch 2, cost  -12425.218\n",
      "Training epoch 3, cost  -12425.218\n",
      "Training epoch 4, cost  -12425.218\n",
      "The code ran for 0.18m\n"
     ]
    }
   ],
   "source": [
    "test_NextWord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorType(int64, scalar)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T.lscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
