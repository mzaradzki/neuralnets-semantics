{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Tesla K80 (CNMeM is disabled)\n"
     ]
    }
   ],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import utils_modified; reload(utils_modified)\n",
    "from utils_modified import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, Embedding, Activation, LSTM, merge, Flatten, Dropout, Lambda\n",
    "from keras.models import Model, Sequential\n",
    "from keras.engine.topology import Merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.layers.convolutional import *\n",
    "from keras.utils.data_utils import get_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 600901\n",
      "total chars: 85\n",
      "nb sequences: 200287\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py\n",
    "path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n",
    "text = open(path).read()#.lower()\n",
    "print('corpus length:', len(text))\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# cut the text in semi-redundant sequences of maxlen characters\n",
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lag = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = [char_indices[c] for c in text]\n",
    "\n",
    "Xs = []\n",
    "for l in range(lag):\n",
    "    cdat = [idx[i+l] for i in xrange(0, len(idx)-1-lag, lag)]\n",
    "    X = np.stack(cdat[:-2])\n",
    "    Xs.append(X)\n",
    "\n",
    "cdat = [idx[i+(l+1)] for i in xrange(0, len(idx)-1-lag, lag)]\n",
    "Y = np.stack(cdat[:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdim = 250\n",
    "\n",
    "inputs = [Input(shape=(1,), dtype='int64') for i in range(lag)]\n",
    "\n",
    "E = Embedding(output_dim=hdim, input_dim=len(chars), input_length=1)\n",
    "\n",
    "Di2h = Dense(hdim, activation='relu')\n",
    "Dh2h = Dense(hdim, activation='relu', init='identity')\n",
    "\n",
    "#hidden = ... CONSTANT ZERO TENSOR IN KERAS ?\n",
    "\n",
    "e = Di2h(Flatten()(E(inputs[0])))\n",
    "hidden = e\n",
    "for i in range(1,lag):\n",
    "    e = Di2h(Flatten()(E(inputs[i])))\n",
    "    hidden = merge([e, Dh2h(hidden)], mode='sum')\n",
    "predictions = Dense(len(chars), activation='softmax')(hidden)\n",
    "\n",
    "model = Model(input=inputs, output=predictions)\n",
    "\n",
    "model.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "100147/100147 [==============================] - 7s - loss: 2.4715 - acc: 0.3039     \n",
      "Epoch 2/12\n",
      "100147/100147 [==============================] - 7s - loss: 2.1777 - acc: 0.3744     \n",
      "Epoch 3/12\n",
      "100147/100147 [==============================] - 7s - loss: 2.0834 - acc: 0.4012     \n",
      "Epoch 4/12\n",
      "100147/100147 [==============================] - 7s - loss: 2.0176 - acc: 0.4197     \n",
      "Epoch 5/12\n",
      "100147/100147 [==============================] - 7s - loss: 1.9637 - acc: 0.4314     \n",
      "Epoch 6/12\n",
      "100147/100147 [==============================] - 7s - loss: 1.9214 - acc: 0.4440     \n",
      "Epoch 7/12\n",
      "100147/100147 [==============================] - 7s - loss: 1.8859 - acc: 0.4537     \n",
      "Epoch 8/12\n",
      "100147/100147 [==============================] - 7s - loss: 1.8544 - acc: 0.4618     \n",
      "Epoch 9/12\n",
      "100147/100147 [==============================] - 7s - loss: 1.8292 - acc: 0.4687     \n",
      "Epoch 10/12\n",
      "100147/100147 [==============================] - 7s - loss: 1.8081 - acc: 0.4736     \n",
      "Epoch 11/12\n",
      "100147/100147 [==============================] - 7s - loss: 1.7883 - acc: 0.4771     \n",
      "Epoch 12/12\n",
      "100147/100147 [==============================] - 7s - loss: 1.7711 - acc: 0.4830     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb661cb090>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.lr = 1e-3\n",
    "model.fit(Xs, to_categorical(Y,len(chars)), batch_size=100, nb_epoch=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_top_next(inp):\n",
    "    pad_inp = (' '*lag)+inp\n",
    "    idxs = [char_indices[c] for c in pad_inp][-lag:]\n",
    "    ps = model.predict([np.array([i]) for i in idxs])\n",
    "    probas = ps[0]/np.sum(ps)\n",
    "    for i in range(5):\n",
    "        index = np.random.choice(range(len(chars)), size=None, replace=True, p=probas)\n",
    "        print(inp+chars[index])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this it\n",
      "this is\n",
      "this is\n",
      "this id\n",
      "this in\n",
      "\n",
      "herseli\n",
      "hersela\n",
      "hersely\n",
      "hersely\n",
      "herselv\n",
      "\n",
      "himself\n",
      "himself\n",
      "himself\n",
      "himself\n",
      "himself\n",
      "\n",
      "morali\n",
      "morali\n",
      "moral \n",
      "moral \n",
      "moralm\n",
      "\n",
      "morality\n",
      "morality\n",
      "morality\n",
      "morality\n",
      "morality\n",
      "\n",
      "knowledgi\n",
      "knowledge\n",
      "knowledge\n",
      "knowledge\n",
      "knowledge\n",
      "\n",
      "logical\n",
      "logical\n",
      "logical\n",
      "logica \n",
      "logical\n",
      "\n",
      "hypothesin\n",
      "hypothesis\n",
      "hypothesit\n",
      "hypothesis\n",
      "hypothesis\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_top_next('this i')\n",
    "show_top_next('hersel')\n",
    "show_top_next('himsel')\n",
    "show_top_next('moral')\n",
    "show_top_next('moralit')\n",
    "show_top_next('knowledg')\n",
    "show_top_next('logica')\n",
    "show_top_next('hypothesi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
