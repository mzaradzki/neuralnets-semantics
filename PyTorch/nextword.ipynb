{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREFER_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = PREFER_CUDA and torch.cuda.is_available()\n",
    "\n",
    "# args.seed\n",
    "batch_size = 100\n",
    "nb_epochs = 5\n",
    "\n",
    "'''kwargs = {'num_workers': 1, 'pin_memory': True} if else {}\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   \n",
    "    batch_size=, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       \n",
    "    batch_size=, shuffle=True, **kwargs)\n",
    "'''\n",
    "\n",
    "vocab_size = 250\n",
    "n_words = 3\n",
    "n_embed = 50\n",
    "n_hidden = 200\n",
    "n_output = vocab_size\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.6940 -0.1874 -0.1818 -0.7801 -0.1634 -1.0847  0.6530  0.3351\n",
      " -1.8435  0.8350  0.3167  1.2730 -0.8385  1.7186  0.6867 -0.9712\n",
      " -0.9578 -0.0720  0.0979 -2.5491 -0.1813  0.3828  2.3271 -0.9195\n",
      "\n",
      "(1 ,.,.) = \n",
      " -1.0836  1.1430 -0.6539 -0.0661  0.4463 -0.0222  1.0801 -1.8776\n",
      " -0.5286  2.1875  0.0653 -0.4930 -0.9506  0.9420  0.6597  0.1213\n",
      " -1.8435  0.8350  0.3167  1.2730 -0.8385  1.7186  0.6867 -0.9712\n",
      "[torch.FloatTensor of size 2x3x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(vocab_size, 8)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = Variable(torch.LongTensor([[11,20,4],[30,10,20],]))\n",
    "print(embedding(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convert word into vectors\n",
    "        # see documentation : http://pytorch.org/docs/nn.html#torch.nn.Embedding\n",
    "        self.W2E = nn.Embedding(vocab_size, n_embed, padding_idx=None, max_norm=None, norm_type=2)\n",
    "        self.E2H = nn.Linear(n_words*n_embed, n_hidden, bias=True)\n",
    "        self.H2O = nn.Linear(n_hidden, n_output, bias=True)\n",
    "        #self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "    def forward(self, words):\n",
    "        wvectors = self.W2E(words)\n",
    "        # WARNING : wvectors may need a reshape operation ???\n",
    "        wvectorsR = wvectors.resize(words.size(0), n_words*n_embed)\n",
    "        h_activation = self.E2H(wvectorsR)\n",
    "        h = F.sigmoid(h_activation)\n",
    "        out_activation = self.H2O(h)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        #return F.log_softmax(x)\n",
    "        return F.softmax(out_activation) # WARNING : or log_softmax ???\n",
    "\n",
    "model = Net()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "utilities.py:30: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  train_input = reshape(data.trainData[0:D, 0:N * M], (D, N, M));\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/numpy/core/fromnumeric.py:224: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  return reshape(newshape, order=order)\n",
      "utilities.py:33: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  train_target = reshape(data.trainData[D, 0:N * M], (1, N, M));\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3, 100, 3725)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utilities import load_data\n",
    "[train_input, train_target, valid_input, valid_target, test_input, test_target, vocab] = load_data(100)\n",
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 250])\n"
     ]
    }
   ],
   "source": [
    "TEST = True\n",
    "if TEST:\n",
    "    words = Variable(torch.LongTensor([[1,4,2], [13,44,3]]))\n",
    "    W2E = nn.Embedding(vocab_size, n_embed, padding_idx=None, max_norm=None, norm_type=2)\n",
    "    E2H = nn.Linear(n_words*n_embed, n_hidden, bias=True)\n",
    "    H2O = nn.Linear(n_hidden, n_output, bias=True)\n",
    "    #\n",
    "    wvectors = W2E(words)\n",
    "    #print(wvectors.size())\n",
    "    wvectorsR = wvectors.resize(words.size(0), n_words*n_embed)\n",
    "    #print(wvectorsR.size())\n",
    "    h_activation = E2H(wvectorsR)\n",
    "    h = F.sigmoid(h_activation)\n",
    "    out_activation = H2O(h)\n",
    "    print(out_activation.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    batch_idx = 0\n",
    "    data = torch.LongTensor(train_input[:,:,batch_idx].T.astype('int'))\n",
    "    print(data.size())\n",
    "    target = torch.LongTensor(train_target[:,:,batch_idx].T.astype('int'))\n",
    "    print(target.size())\n",
    "    if use_cuda:\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx in range(10):\n",
    "        data = torch.LongTensor(train_input[:,:,batch_idx].T.astype('int'))\n",
    "        print(data.size())\n",
    "        target = torch.LongTensor(train_target[:,:,batch_idx].T.astype('int'))\n",
    "        print(target.size())\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #loss = F.nll_loss(output, target)\n",
    "        loss = F.cross_entropy(output, target, size_average=True) # WARNING : target one_hot_encoding\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "for epoch in range(1, nb_epochs + 1):\n",
    "    #train(epoch)\n",
    "    #test(epoch)\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3])\n",
      "torch.Size([100, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "multi-target not supported at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.6_1485015011449/work/torch/lib/THNN/generic/ClassNLLCriterion.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-91189b3a0f94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-863ad9b065da>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m#loss = F.nll_loss(output, target)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# WARNING : target one_hot_encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average)\u001b[0m\n\u001b[0;32m    449\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \"\"\"\n\u001b[1;32m--> 451\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/nn/functional.pyc\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \"\"\"\n\u001b[1;32m--> 419\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/nn/_functions/thnn/auto.pyc\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         getattr(self._backend, update_output.name)(self._backend.library_state, input, target,\n\u001b[1;32m---> 41\u001b[1;33m             output, *self.additional_args)\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: multi-target not supported at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.6_1485015011449/work/torch/lib/THNN/generic/ClassNLLCriterion.c:20"
     ]
    }
   ],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
