{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PREFER_CUDA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "use_cuda = PREFER_CUDA and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "if not(use_cuda == PREFER_CUDA):\n",
    "    print('CUDA SETUP NOT AS EXCEPTED')\n",
    "else:\n",
    "    print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# args.seed\n",
    "batch_size = 100\n",
    "nb_epochs = 5\n",
    "\n",
    "vocab_size = 250\n",
    "n_words = 3\n",
    "n_embed = 50\n",
    "n_hidden = 200\n",
    "n_output = vocab_size\n",
    "\n",
    "learning_rate = 0.1\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.7362  2.0128  1.5946 -1.1494  0.5511 -1.8720  0.6247 -1.3850\n",
      " -1.0243 -1.2624 -1.0411 -2.0303 -0.5651 -0.2828 -1.5301  1.5863\n",
      "  0.3019  0.5329  1.3229 -0.7000  0.2968  0.3983  1.2594 -1.1148\n",
      "\n",
      "(1 ,.,.) = \n",
      " -2.1298  1.5368 -0.4870 -1.4527 -0.5174 -1.2402  0.6551  1.4828\n",
      "  1.6022  0.6770  1.0320  0.0512 -0.9889 -0.0280  1.2209 -0.3387\n",
      " -1.0243 -1.2624 -1.0411 -2.0303 -0.5651 -0.2828 -1.5301  1.5863\n",
      "[torch.FloatTensor of size 2x3x8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "embedding = nn.Embedding(vocab_size, 8)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = Variable(torch.LongTensor([[11,20,4],[30,10,20],]))\n",
    "print(embedding(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "class NLLLoss(_WeightedLoss):\n",
    "    \"\"\"The negative log likelihood loss. It is useful to train a classification problem with n classes\n",
    "\n",
    "    If provided, the optional argument `weights` should be a 1D Tensor assigning\n",
    "    weight to each of the classes.\n",
    "\n",
    "    This is particularly useful when you have an unbalanced training set.\n",
    "\n",
    "    The input given through a forward call is expected to contain log-probabilities\n",
    "    of each class: input has to be a 2D Tensor of size `(minibatch, n)`\n",
    "\n",
    "    Obtaining log-probabilities in a neural network is easily achieved by\n",
    "    adding a  `LogSoftmax`  layer in the last layer of your network.\n",
    "\n",
    "    You may use `CrossEntropyLoss`  instead, if you prefer not to add an extra layer.\n",
    "\n",
    "    The target that this loss expects is a class index `(0 to N-1, where N = number of classes)`\n",
    "\n",
    "    The loss can be described as::\n",
    "\n",
    "        loss(x, class) = -x[class]\n",
    "'''\n",
    "loss_function = nn.NLLLoss()\n",
    "#loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # convert word into vectors\n",
    "        # see documentation : http://pytorch.org/docs/nn.html#torch.nn.Embedding\n",
    "        self.W2E = nn.Embedding(vocab_size, n_embed, padding_idx=None, max_norm=None, norm_type=2)\n",
    "        self.E2H = nn.Linear(n_words*n_embed, n_hidden, bias=True)\n",
    "        self.H2O = nn.Linear(n_hidden, n_output, bias=True)\n",
    "        #self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "    def forward(self, words):\n",
    "        wvectors = self.W2E(words)\n",
    "        # WARNING : wvectors may need a reshape operation ???\n",
    "        wvectorsR = wvectors.resize(words.size(0), n_words*n_embed)\n",
    "        h_activation = self.E2H(wvectorsR)\n",
    "        h = F.sigmoid(h_activation)\n",
    "        out_activation = self.H2O(h)\n",
    "        #x = F.relu(self.fc1(x))\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        #return F.softmax(out_activation) # WARNING : or log_softmax ???\n",
    "        return F.log_softmax(out_activation) # WARNING : or log_softmax ???\n",
    "\n",
    "model = Net()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 3725)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utilities import load_data\n",
    "[train_input, train_target, valid_input, valid_target, test_input, test_target, vocab] = load_data(100)\n",
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 250])\n"
     ]
    }
   ],
   "source": [
    "TEST = True\n",
    "if TEST:\n",
    "    words = Variable(torch.LongTensor([[1,4,2], [13,44,3]]))\n",
    "    W2E = nn.Embedding(vocab_size, n_embed, padding_idx=None, max_norm=None, norm_type=2)\n",
    "    E2H = nn.Linear(n_words*n_embed, n_hidden, bias=True)\n",
    "    H2O = nn.Linear(n_hidden, n_output, bias=True)\n",
    "    #\n",
    "    wvectors = W2E(words)\n",
    "    #print(wvectors.size())\n",
    "    wvectorsR = wvectors.resize(words.size(0), n_words*n_embed)\n",
    "    #print(wvectorsR.size())\n",
    "    h_activation = E2H(wvectorsR)\n",
    "    h = F.sigmoid(h_activation)\n",
    "    out_activation = H2O(h)\n",
    "    print(out_activation.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3])\n",
      "torch.Size([100, 1])\n"
     ]
    }
   ],
   "source": [
    "if TEST:\n",
    "    batch_idx = 0\n",
    "    data = torch.LongTensor(train_input[:,:,batch_idx].T.astype('int'))\n",
    "    print(data.size())\n",
    "    target = torch.LongTensor(train_target[:,:,batch_idx].T.astype('int'))\n",
    "    print(target.size())\n",
    "    if use_cuda:\n",
    "        data = data.cuda()\n",
    "        target = target.cuda()\n",
    "    data, target = Variable(data), Variable(target)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "print(train_target.T.astype('int').min())\n",
    "print(train_target.T.astype('int').max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb_batches = 50\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx in range(nb_batches):\n",
    "        data = torch.LongTensor(train_input[:,:,batch_idx].T.astype('int'))\n",
    "        target = torch.LongTensor(np.squeeze(train_target[:,:,batch_idx].T.astype('int'))) # WARNING : squeeze\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #print(data.size())\n",
    "        #print(target.size())\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #print(output.size())\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if ((batch_idx % 10 == 0) or (batch_idx == nb_batches-1)):\n",
    "            print('Train Epoch: {} [{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WARNING : code to complete based on train function\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0]\tLoss: 1.817124\n",
      "Train Epoch: 0 [1000]\tLoss: 1.772211\n",
      "Train Epoch: 0 [2000]\tLoss: 1.641063\n",
      "Train Epoch: 0 [3000]\tLoss: 1.661208\n",
      "Train Epoch: 0 [4000]\tLoss: 1.976789\n",
      "Train Epoch: 0 [4900]\tLoss: 1.597174\n",
      "Train Epoch: 1 [0]\tLoss: 1.718200\n",
      "Train Epoch: 1 [1000]\tLoss: 1.683529\n",
      "Train Epoch: 1 [2000]\tLoss: 1.562136\n",
      "Train Epoch: 1 [3000]\tLoss: 1.591454\n",
      "Train Epoch: 1 [4000]\tLoss: 1.869167\n",
      "Train Epoch: 1 [4900]\tLoss: 1.514817\n",
      "Train Epoch: 2 [0]\tLoss: 1.628178\n",
      "Train Epoch: 2 [1000]\tLoss: 1.596057\n",
      "Train Epoch: 2 [2000]\tLoss: 1.483405\n",
      "Train Epoch: 2 [3000]\tLoss: 1.512661\n",
      "Train Epoch: 2 [4000]\tLoss: 1.765398\n",
      "Train Epoch: 2 [4900]\tLoss: 1.437800\n",
      "Train Epoch: 3 [0]\tLoss: 1.546964\n",
      "Train Epoch: 3 [1000]\tLoss: 1.510065\n",
      "Train Epoch: 3 [2000]\tLoss: 1.406760\n",
      "Train Epoch: 3 [3000]\tLoss: 1.434295\n",
      "Train Epoch: 3 [4000]\tLoss: 1.668755\n",
      "Train Epoch: 3 [4900]\tLoss: 1.365084\n",
      "Train Epoch: 4 [0]\tLoss: 1.471082\n",
      "Train Epoch: 4 [1000]\tLoss: 1.429088\n",
      "Train Epoch: 4 [2000]\tLoss: 1.335451\n",
      "Train Epoch: 4 [3000]\tLoss: 1.361067\n",
      "Train Epoch: 4 [4000]\tLoss: 1.578842\n",
      "Train Epoch: 4 [4900]\tLoss: 1.298127\n",
      "Train Epoch: 5 [0]\tLoss: 1.398768\n",
      "Train Epoch: 5 [1000]\tLoss: 1.356743\n",
      "Train Epoch: 5 [2000]\tLoss: 1.263676\n",
      "Train Epoch: 5 [3000]\tLoss: 1.294102\n",
      "Train Epoch: 5 [4000]\tLoss: 1.495021\n",
      "Train Epoch: 5 [4900]\tLoss: 1.247637\n",
      "Train Epoch: 6 [0]\tLoss: 1.336871\n",
      "Train Epoch: 6 [1000]\tLoss: 1.291261\n",
      "Train Epoch: 6 [2000]\tLoss: 1.188950\n",
      "Train Epoch: 6 [3000]\tLoss: 1.240317\n",
      "Train Epoch: 6 [4000]\tLoss: 1.406362\n",
      "Train Epoch: 6 [4900]\tLoss: 1.225997\n",
      "Train Epoch: 7 [0]\tLoss: 1.304179\n",
      "Train Epoch: 7 [1000]\tLoss: 1.217298\n",
      "Train Epoch: 7 [2000]\tLoss: 1.149035\n",
      "Train Epoch: 7 [3000]\tLoss: 1.191965\n",
      "Train Epoch: 7 [4000]\tLoss: 1.308143\n",
      "Train Epoch: 7 [4900]\tLoss: 1.177672\n",
      "Train Epoch: 8 [0]\tLoss: 1.288354\n",
      "Train Epoch: 8 [1000]\tLoss: 1.137206\n",
      "Train Epoch: 8 [2000]\tLoss: 1.142865\n",
      "Train Epoch: 8 [3000]\tLoss: 1.125083\n",
      "Train Epoch: 8 [4000]\tLoss: 1.259761\n",
      "Train Epoch: 8 [4900]\tLoss: 1.059481\n",
      "Train Epoch: 9 [0]\tLoss: 1.185441\n",
      "Train Epoch: 9 [1000]\tLoss: 1.098067\n",
      "Train Epoch: 9 [2000]\tLoss: 1.046275\n",
      "Train Epoch: 9 [3000]\tLoss: 1.090499\n",
      "Train Epoch: 9 [4000]\tLoss: 1.198059\n",
      "Train Epoch: 9 [4900]\tLoss: 1.011349\n"
     ]
    }
   ],
   "source": [
    "for e in range(30):\n",
    "    train(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
