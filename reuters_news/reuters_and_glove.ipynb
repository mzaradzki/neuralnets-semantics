{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from theano.sandbox import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math, os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Embedding, Activation, Reshape, Permute, merge\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.layers.convolutional import *\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Reuters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_path = '../data/colearn/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb, reuters\n",
    "idx = reuters.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'the', u'of', u'to', u'in', u'said', u'and', u'a', u'mln', u'3', u'for']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_arr = sorted(idx, key=idx.get)\n",
    "idx_arr[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30979"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2word = {v: k for k, v in idx.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'said', 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5\n",
    "(idx2word[n], idx[idx2word[n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import reuters\n",
    "\n",
    "# WARNING : this function has a bug when oov_char is None\n",
    "# source : https://raw.githubusercontent.com/fchollet/keras/master/keras/datasets/reuters.py\n",
    "(x_train, labels_train), (x_test, labels_test) = reuters.load_data(path=\"reuters.npz\",\n",
    "                                                         num_words=None,\n",
    "                                                         skip_top=0,\n",
    "                                                         maxlen=None,\n",
    "                                                         test_split=0.2,\n",
    "                                                         seed=113,\n",
    "                                                         start_char=None,\n",
    "                                                         oov_char=0,\n",
    "                                                         index_from=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L = []\n",
    "L.extend(labels_train)\n",
    "L.extend(labels_test)\n",
    "nb_categories = len(set(L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 55, 138)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x_train contains lists of variable length were words are represented by indices\n",
    "len(x_train[0]), len(x_train[1]), len(x_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'mcgrath rentcorp said as a result of its december acquisition of space co it expects earnings per share in 1987 of 1 15 to 1 30 dlrs per share up from 70 cts in 1986 the company said pretax net should rise to nine to 10 mln dlrs from six mln dlrs in 1986 and rental operation revenues to 19 to 22 mln dlrs from 12 5 mln dlrs it said cash flow per share this year should be 2 50 to three dlrs reuter 3'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrds = [idx2word[n] for n in x_train[0]]\n",
    "' '.join(wrds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 2500\n",
    "maxlen = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def paddedset(orig_set):\n",
    "    adj_set = [[min(n,vocab_size-1) for n in sq] for sq in orig_set]\n",
    "    return pad_sequences(adj_set, maxlen=maxlen, dtype='int32', padding='pre', truncating='post', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_pad = paddedset(x_train)\n",
    "x_test_pad = paddedset(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "[27592, 28839, 5, 40, 7, 444, 2, 22]\n",
      "[2499, 2499, 5, 40, 7, 444, 2, 22]\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[0]))\n",
    "print(x_train[0][0:8])\n",
    "print([n for n in x_train_pad[0] if n>0][0:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn Reuters dataset into CBOW 4-grams\n",
    "### Remember that CBOW is an unsupervised semantic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cbow_lag = 2 # (lag, _, lag) => t\n",
    "\n",
    "def f(inputs=x_train, size=1000):\n",
    "    outputs = []\n",
    "    for sentence in x_train[0:size]:\n",
    "        for i in range(cbow_lag, len(sentence)-cbow_lag):\n",
    "            outputs.append( (sentence[i-cbow_lag:i] + sentence[i+1:i+cbow_lag+1], sentence[i]) )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "709702"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_train = f(x_train, size=5000)\n",
    "len(cbow_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "289222"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cbow_test = f(x_test, size=2000)\n",
    "len(cbow_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391752\n"
     ]
    }
   ],
   "source": [
    "np.random.shuffle(cbow_train)\n",
    "cbow_x_train = [x for x, y in cbow_train if max(x)<vocab_size and y<vocab_size]\n",
    "cbow_labels_train = [y for x, y in cbow_train if max(x)<vocab_size and y<vocab_size]\n",
    "print(len(cbow_x_train))\n",
    "\n",
    "np.random.shuffle(cbow_test)\n",
    "cbow_x_test = [x for x, y in cbow_test if max(x)<vocab_size and y<vocab_size]\n",
    "cbow_labels_test = [y for x, y in cbow_test if max(x)<vocab_size and y<vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe embedding pre-trained weights\n",
    "#### Command to use :\n",
    "wget http://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "GLOVE_DIR = '../data/glove6B/'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biennials (100,)\n"
     ]
    }
   ],
   "source": [
    "k = embeddings_index.keys()[0]\n",
    "v = embeddings_index[k]\n",
    "print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newwords = embeddings_index.keys()\n",
    "count = 0\n",
    "for i in range(1,vocab_size):\n",
    "    if not(idx2word[i] in newwords):\n",
    "        count+=1\n",
    "        #print(idx2word[i])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "#for word, i in idx2word[i]:#word_index.items():\n",
    "for i in range(1,vocab_size):\n",
    "    word = idx2word[i]\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained = True # use Glove weights or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Turning words into embedded vectors\n",
    "#    Documentation : https://keras.io/layers/embeddings/\n",
    "#    The model will take as Input an integer matrix of size (batch_size, n_words).\n",
    "#    The largest integer (i.e. word index) in the input should be no larger than vocab_size.\n",
    "#    Now model.output_shape == (None, n_words, n_embed), where None is the batch dimension.\n",
    "if pretrained:\n",
    "    EMBEDDING = Embedding(vocab_size, EMBEDDING_DIM, weights=[embedding_matrix]) # WARNING : input_length ?\n",
    "else:\n",
    "    EMBEDDING = Embedding(vocab_size, EMBEDDING_DIM) # WARNING : input_length ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW model to infer semantics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_words = 2*cbow_lag # we use LAG words before and LAG words after as inputs\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelWRD = Sequential()\n",
    "\n",
    "# First layer is a dummy-permutation = Identity to specify input shape\n",
    "modelWRD.add( Permute((1,), input_shape=(n_words,)) ) # WARNING : axis 0 is the sample dim\n",
    "\n",
    "modelWRD.add(EMBEDDING)\n",
    "\n",
    "modelWRD.add(Lambda(lambda x : K.sum(x,axis=1), output_shape=(EMBEDDING_DIM,)))\n",
    "\n",
    "modelWRD.add(Dense(vocab_size, input_shape=(EMBEDDING_DIM,), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2500)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelWRD.predict(np.ones((5,n_words))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_10 (Permute)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 4, 100)            250000    \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2500)              252500    \n",
      "=================================================================\n",
      "Total params: 502,500.0\n",
      "Trainable params: 502,500.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelWRD.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First round of training : keeps embedding matrix fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING.trainable = False # WARNING : needs re-compiling to be effective\n",
    "modelWRD.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "391752/391752 [==============================] - 38s - loss: 4.6076 - acc: 0.1826    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e1f99e210>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelWRD.fit(cbow_x_train, to_categorical(cbow_labels_train,vocab_size),\n",
    "             batch_size=64,\n",
    "             epochs=3,\n",
    "             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second round of training : tune the embedding matrix too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING.trainable = True # WARNING : needs re-compiling to be effective\n",
    "modelWRD.compile(optimizer=Adam(1e-3), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "391752/391752 [==============================] - 41s - loss: 3.5134 - acc: 0.3134    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e2583fb10>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelWRD.optimizer.lr = 1e-4\n",
    "modelWRD.fit(cbow_x_train, to_categorical(cbow_labels_train,vocab_size),\n",
    "             batch_size=64,\n",
    "             epochs=1,\n",
    "             shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction model of text category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EMBEDDING.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WARNING : fix the bug with MaxPooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Neural Network architecture defined here :\n",
    "#     https://quid.com/feed/how-quid-uses-deep-learning-with-small-data\n",
    "graph_in = Input(shape=(maxlen, EMBEDDING_DIM))\n",
    "\n",
    "convs = []\n",
    "for w in range(1,4): # the convolution window width\n",
    "    conv = Conv1D(filters=300,\n",
    "                  kernel_size=w,\n",
    "                  padding='valid',\n",
    "                  activation='relu',\n",
    "                  strides=1)(graph_in)\n",
    "    # keras.layers.pooling.MaxPooling1D(pool_size=2, strides=None, padding='valid')\n",
    "    pool = conv#MaxPooling1D(pool_size=2)(conv)\n",
    "    flatten = Flatten()(pool)\n",
    "    convs.append(flatten)\n",
    "\n",
    "out = Concatenate(axis=-1)(convs) # WARNING : check axis and dimension\n",
    "graph = Model(inputs=graph_in, outputs=out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelTXT = Sequential()\n",
    "\n",
    "# First layer is a dummy-permutation = Identity to specify input shape\n",
    "modelTXT.add( Permute((1,), input_shape=(maxlen,)) ) # WARNING : axis 0 is the sample dim\n",
    "\n",
    "if True:\n",
    "    modelTXT.add(EMBEDDING)\n",
    "else:\n",
    "    # INFO : may not be necessary but cleared this way, otherwise shape in .summary is not clear\n",
    "    EMBEDDING_TXT = Embedding(vocab_size, EMBEDDING_DIM, weights=EMBEDDING.get_weights())\n",
    "    EMBEDDING_TXT.trainable = False\n",
    "    modelTXT.add(EMBEDDING_TXT)\n",
    "\n",
    "modelTXT.add(graph)\n",
    "\n",
    "modelTXT.add(Dense(300))\n",
    "modelTXT.add(Dropout(0.5))\n",
    "modelTXT.add(Activation('relu'))\n",
    "modelTXT.add(Dense(100))\n",
    "modelTXT.add(Dropout(0.5))\n",
    "modelTXT.add(Activation('relu'))\n",
    "modelTXT.add(Dense(nb_categories))\n",
    "modelTXT.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute_8 (Permute)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      multiple                  250000    \n",
      "_________________________________________________________________\n",
      "model_3 (Model)              (None, 449100)            180900    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 300)               134730300 \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 46)                4646      \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 46)                0         \n",
      "=================================================================\n",
      "Total params: 135,195,946.0\n",
      "Trainable params: 134,945,946.0\n",
      "Non-trainable params: 250,000.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modelTXT.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "modelTXT.compile(optimizer=Adam(), loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "8982/8982 [==============================] - 90s - loss: 1.5774 - acc: 0.6277    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5e19c81950>"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTXT.fit(x_train_pad,\n",
    "             to_categorical(labels_train, nb_categories),\n",
    "             batch_size=100,\n",
    "             #validation_data=(x_test_pad, labels_test),\n",
    "             epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 46)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTXT.predict(x_train_pad[0:5]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
